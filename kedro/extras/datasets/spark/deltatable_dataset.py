"""``AbstractVersionedDataSet`` implementation to access DeltaTables using
``delta-spark``
"""
import logging
from typing import Any, Dict

from delta.tables import DeltaTable

from kedro.extras.datasets.spark import SparkDataSet
from kedro.io.core import Version

logger = logging.getLogger(__name__)


class DeltaTableDataset(SparkDataSet):
    """``DeltaTableDataset`` loads data into DeltaTable objects.

        Example adding a catalog entry with
        `YAML API <https://kedro.readthedocs.io/en/stable/05_data/\
            01_data_catalog.html#using-the-data-catalog-with-the-yaml-api>`_:

        .. code-block:: yaml

            >>> weather:
            >>>   type: spark.SparkDataSet
            >>>   filepath: data/02_intermediate/data.parquet
            >>>   file_format: "delta"
            >>>   load_args:
            >>>     header: True
            >>>     inferSchema: True
            >>>   save_args:
            >>>     sep: "|"
            >>>     header: True
            >>>
            >>> weather_cleaned:
            >>>   type: spark.DeltaTableDataset
            >>>   filepath: data/02_intermediate/data.parquet

        Example using Python API:
        ::

            >>> from pyspark.sql import SparkSession
            >>> from pyspark.sql.types import (StructField, StringType,
            >>>                                IntegerType, StructType)
            >>>
            >>> from kedro.extras.datasets.spark import DeltaTableDataset, SparkDataSet
            >>>
            >>> schema = StructType([StructField("name", StringType(), True),
            >>>                      StructField("age", IntegerType(), True)])
            >>>
            >>> data = [('Alex', 31), ('Bob', 12), ('Clarke', 65), ('Dave', 29)]
            >>>
            >>> spark_df = SparkSession.builder.getOrCreate()\
            >>>                        .createDataFrame(data, schema)
            >>>
            >>> data_set = SparkDataSet(filepath="test_data")
            >>> data_set.save(spark_df)
            >>> deltatable_dataset = DeltaTableDataset(filepath="test_data")
            >>> delta_table = deltatable_dataset.load()
            >>>
            >>> delta_table.update()
        """

    def __init__(  # pylint: disable=too-many-arguments
        self,
        filepath: str,
        load_args: Dict[str, Any] = None,
        save_args: Dict[str, Any] = None,
        version: Version = None,
        credentials: Dict[str, Any] = None,
    ) -> None:
        """Creates a new instance of ``DeltaTableDataset``.

        Args:
            filepath: Filepath in POSIX format to a Spark dataframe. When using Databricks
                and working with data written to mount path points,
                specify ``filepath``s for (versioned) ``SparkDataSet``s
                starting with ``/dbfs/mnt``.
            load_args: Load args passed to Spark DataFrameReader load method.
                It is dependent on the selected file format. You can find
                a list of read options for each supported format
                in Spark DataFrame read documentation:
                https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html
            save_args: Save args passed to Spark DataFrame write options.
                Similar to load_args this is dependent on the selected file
                format. You can pass ``mode`` and ``partitionBy`` to specify
                your overwrite mode and partitioning respectively. You can find
                a list of options for each format in Spark DataFrame
                write documentation:
                https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html
            version: If specified, should be an instance of
                ``kedro.io.core.Version``. If its ``load`` attribute is
                None, the latest version will be loaded. If its ``save``
                attribute is None, save version will be autogenerated.
            credentials: Credentials to access the S3 bucket, such as
                ``key``, ``secret``, if ``filepath`` prefix is ``s3a://`` or ``s3n://``.
                Optional keyword arguments passed to ``hdfs.client.InsecureClient``
                if ``filepath`` prefix is ``hdfs://``. Ignored otherwise.
        """
        super().__init__(
            filepath=filepath,
            file_format="delta",
            load_args=load_args,
            save_args=save_args,
            version=version,
            credentials=credentials,
        )

    def _load(self) -> DeltaTable:
        load_path = self._fs_prefix + str(self._get_load_path())
        return DeltaTable.forPath(self._get_spark(), load_path)

    def _save(self, data: Any) -> None:
        logger.info(
            "Saving was performed on `DeltaTable` object within the context of the node function"
        )
